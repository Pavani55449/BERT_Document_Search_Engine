{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       num                                               name  \\\n",
      "0  9180533                         the.message.(1976).eng.1cd   \n",
      "1  9180583  here.comes.the.grump.s01.e09.joltin.jack.in.bo...   \n",
      "2  9180592    yumis.cells.s02.e13.episode.2.13.(2022).eng.1cd   \n",
      "3  9180594    yumis.cells.s02.e14.episode.2.14.(2022).eng.1cd   \n",
      "4  9180600                              broker.(2022).eng.1cd   \n",
      "\n",
      "                                   extracted_content  \\\n",
      "0  watch video online open subtitle free browser ...   \n",
      "1  ah there 's princess dawn terry blooney looney...   \n",
      "2  yumi 's cell episode extremely polite yumi yum...   \n",
      "3  watch video online open subtitle free browser ...   \n",
      "4  watch video online open subtitle free browser ...   \n",
      "\n",
      "                                     chunked_content  \n",
      "0  [['watch', 'video', 'online', 'open', 'subtitl...  \n",
      "1  [['ah', 'there', \"'s\", 'princess', 'dawn', 'te...  \n",
      "2  [['yumi', \"'s\", 'cell', 'episode', 'extremely'...  \n",
      "3  [['watch', 'video', 'online', 'open', 'subtitl...  \n",
      "4  [['watch', 'video', 'online', 'open', 'subtitl...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = r\"D:\\ML\\Innomatics_Research_Lab_Internship\\task8_SearchEngine\\df_cleaned_chunks12-04.csv\"\n",
    "\n",
    "# Define the range of rows to read\n",
    "start_row = 0\n",
    "end_row = 150\n",
    "\n",
    "# Calculate the number of rows to read\n",
    "rows_to_read = end_row - start_row\n",
    "\n",
    "# Read rows 0 to 45000 from the CSV file\n",
    "df_sample = pd.read_csv(file_path, skiprows=start_row, nrows=rows_to_read)\n",
    "\n",
    "# Display the first few rows of the sampled DataFrame\n",
    "print(df_sample.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try above method to get rows from particular rows or we can also try below method to take random rows according to your required percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       num                                               name  \\\n",
      "0  9181571  trying.s02.e02.the.sun.on.your.back.(2021).eng...   \n",
      "1  9181886  doctor.lawyer.s01.e14.episode.1.14.(2022).eng.1cd   \n",
      "2  9181932  alchemy.of.souls.s01.e11.episode.1.11.(2022).e...   \n",
      "3  9181967    the.governor.s01.e06.episode.1.6.(1995).eng.1cd   \n",
      "4  9181995  american.experience.s33.e07.the.blinding.of.is...   \n",
      "\n",
      "                                   extracted_content  \\\n",
      "0  nikki sigh even know we 're i 've already got ...   \n",
      "1  name people incident background drama fictiona...   \n",
      "2  enjoy vod high quality tv get live tv movie sh...   \n",
      "3  cell door clanging braithwaite hey i 'm onto k...   \n",
      "4  watch video online open subtitle free browser ...   \n",
      "\n",
      "                                     chunked_content  \n",
      "0  [['nikki', 'sigh', 'even', 'know', 'we', \"'re\"...  \n",
      "1  [['name', 'people', 'incident', 'background', ...  \n",
      "2  [['enjoy', 'vod', 'high', 'quality', 'tv', 'ge...  \n",
      "3  [['cell', 'door', 'clanging', 'braithwaite', '...  \n",
      "4  [['watch', 'video', 'online', 'open', 'subtitl...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Define the file path\n",
    "file_path = r\"D:\\ML\\Innomatics_Research_Lab_Internship\\task8_SearchEngine\\df_cleaned_chunks12-04.csv\"\n",
    "\n",
    "# Read the total number of rows in the CSV file (you may need to adjust engine='python' if you encounter issues)\n",
    "total_rows = sum(1 for line in open(file_path))\n",
    "\n",
    "# Calculate approximately 30% of the total rows\n",
    "sample_size = int(0.1 * total_rows)\n",
    "\n",
    "# Randomly select rows from the CSV file\n",
    "# Skip rows that are not part of the random sample\n",
    "skip_rows = sorted(random.sample(range(1, total_rows + 1), total_rows - sample_size))\n",
    "\n",
    "# Read the CSV file, skipping the rows that are not part of the random sample\n",
    "df_sample = pd.read_csv(file_path, skiprows=skip_rows)\n",
    "\n",
    "# Display the first few rows of the sampled DataFrame\n",
    "print(df_sample.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8249, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['westworld.s04.e05.zhuangzi.(2022).eng.1cd', 'treasure.city.(2020).eng.1cd', 'a.night.in.97.(2020).eng.1cd', 'the.roundup.(2022).eng.1cd', 'rudrabinar.obhishaap.s02.e06.jogphawl.(2022).eng.1cd', 'my.unfamiliar.family.s01.e03.episode.1.3.(2020).eng.1cd', 'the.governor.s01.e05.episode.1.5.(1995).eng.1cd', 'avaete.semente.da.vinganca.(1985).eng.1cd', 'flowers.in.the.attic.the.origin.s01.e03.part.three.the.murderer.(2022).eng.1cd', 'trying.s02.e03.big.heads.(2021).eng.1cd']\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to persist data to SQLite database\n",
    "def persist_to_database(database_path, data):\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS records\n",
    "                 (num TEXT, name TEXT, embeddings TEXT)''')\n",
    "    \n",
    "    for record in data:\n",
    "        num = record['num']\n",
    "        name = record['name']\n",
    "        embeddings = json.dumps(record['embeddings'])  # Convert embeddings list to JSON string\n",
    "        c.execute(\"INSERT INTO records (num, name, embeddings) VALUES (?, ?, ?)\", (num, name, embeddings))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Initialize BERT model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "start_index = 100\n",
    "end_index = 150\n",
    "# Example DataFrame with columns: num, name, clean_content\n",
    "# df1 = df.head(150)\n",
    "df1=df.iloc[start_index:end_index].copy()\n",
    "\n",
    "# Chunk size and overlap\n",
    "chunk_size = 512  # Adjust this according to your needs\n",
    "overlap_size = 100  # Adjust this according to your needs\n",
    "\n",
    "# Example data processing, chunking, and encoding\n",
    "data_to_persist = []\n",
    "for index, row in df1.iterrows():\n",
    "    num = row['num']\n",
    "    name = row['name']\n",
    "    clean_content = row['extracted_content']\n",
    "    chunks = [clean_content[i:i+chunk_size] for i in range(0, len(clean_content), chunk_size - overlap_size)]\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embeddings = model.encode([chunk])[0].tolist()\n",
    "        data_to_persist.append({'num': f\"{num}_chunk{i+1}\", 'name': name, 'embeddings': embeddings})\n",
    "\n",
    "# Database path\n",
    "database_path = 'D:\\\\ML\\\\Innomatics_Research_Lab_Internship\\\\chroma_50.sqlite3'\n",
    "\n",
    "# Persist data to SQLite database\n",
    "persist_to_database(database_path, data_to_persist)\n",
    "\n",
    "# Function to retrieve top 10 similar records based on query\n",
    "def get_top_10_unique_names(query, database_path):\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    query_embedding = model.encode([query])\n",
    "    similarities = []\n",
    "    c.execute(\"SELECT * FROM records\")\n",
    "    for row in c.fetchall():\n",
    "        record_num, record_name, record_embeddings = row\n",
    "        embeddings = json.loads(record_embeddings)\n",
    "        embeddings = np.array(embeddings).reshape(1, -1)\n",
    "        similarity = cosine_similarity(query_embedding, embeddings)[0][0]\n",
    "        similarities.append((record_name, similarity))\n",
    "    \n",
    "    # Sort by similarity score and extract unique names\n",
    "    sorted_names = [name for name, _ in sorted(similarities, key=lambda x: x[1], reverse=True)]\n",
    "    unique_names = []\n",
    "    for name in sorted_names:\n",
    "        if name not in unique_names:\n",
    "            unique_names.append(name)\n",
    "            if len(unique_names) == 10:\n",
    "                break\n",
    "    \n",
    "    conn.close()\n",
    "    return unique_names\n",
    "\n",
    "# Example usage\n",
    "query = \"   \"\n",
    "top_10_unique_names = get_top_10_unique_names(query, database_path)\n",
    "print(top_10_unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sereng_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
